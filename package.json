{
  "name": "bare-llama-cpp",
  "version": "0.1.0",
  "description": "llama.cpp bindings for Bare",
  "main": "index.js",
  "addon": true,
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/CameronTofer/bare-llama.cpp.git"
  },
  "bugs": {
    "url": "https://github.com/CameronTofer/bare-llama.cpp/issues"
  },
  "homepage": "https://github.com/CameronTofer/bare-llama.cpp",
  "keywords": [
    "llama",
    "llama.cpp",
    "llm",
    "ai",
    "inference",
    "bare",
    "native",
    "gpu",
    "metal",
    "cuda"
  ],
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "fs/*": {
      "bare": "bare-fs/*",
      "default": "fs/*"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    },
    "os": {
      "bare": "bare-os",
      "default": "os"
    }
  },
  "engines": {
    "bare": ">=1.16.0"
  },
  "files": [
    "index.js",
    "binding.js",
    "binding.cpp",
    "prebuilds",
    "CMakeLists.txt"
  ],
  "scripts": {
    "clean": "rm -rf build prebuilds",
    "prepare": "test -d vendor/llama.cpp/.git && bare-make generate && bare-make build && bare-make install || true",
    "update-llama.cpp": "git submodule update --remote vendor/llama.cpp",
    "test": "npm run test:bare",
    "test:bare": "brittle-bare test/*.js",
    "test:node": "brittle test/*.js",
    "bench": "bare bench/run.js"
  },
  "dependencies": {
    "require-addon": "^1.0.0"
  },
  "devDependencies": {
    "@huggingface/jinja": "^0.5.5",
    "bare-fs": "^4.5.3",
    "bare-os": "^3.6.2",
    "bare-path": "^3.0.0",
    "brittle": "^3.19.1",
    "bare-compat-napi": "^1.3.5",
    "cmake-bare": "^1.1.2",
    "cmake-napi": "^1.2.1",
    "paparam": "^1.10.0"
  }
}
